{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytrends.request import TrendReq\n",
    "import id_mappings\n",
    "import translations\n",
    "\n",
    "#Setup\n",
    "curr_dir = os.getcwd()\n",
    "pytrend = TrendReq(timeout=(10,60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema for raw data per country\n",
    "\n",
    "#    country                                str\n",
    "#    region_name                            str\n",
    "#    local_id                               str\n",
    "#    iso_id                                 str\n",
    "#    date                              datetime\n",
    "#    population                             int\n",
    "#    daily_cases                            int\n",
    "#    daily_deaths                           int\n",
    "#    total_cases                            int\n",
    "#    total_deaths                           int\n",
    "#    loss_smell                         float64\n",
    "#    loss_smell_daily_weighting         float64\n",
    "#    sense_smell                          int64\n",
    "#    sense_smell_daily_weighting          int64\n",
    "#    loss_taste                         float64\n",
    "#    loss_taste_daily_weighting         float64\n",
    "#    sense_taste                          int64\n",
    "#    sense_taste_daily_weighting          int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Italian COVID data uses regional coding system rather than ISO codes. See id_mappings.\n",
    "#Trentino-Alto Adige (local_iod: 04) is subdivided into 2 regions (local_ids: 21 and 22)...\n",
    "#...These subregions need to be mapped to local_id: 04 and grouped\n",
    "#First date of recording at 224 cases: 2020-02-24\n",
    "\n",
    "def italy_covid():\n",
    "    italy_file_path = os.path.join(curr_dir, 'raw_covid_data/italy/italy_covid_raw.csv')\n",
    "    result = pd.read_csv(italy_file_path, index_col=None, usecols=['data', 'codice_regione',\\\n",
    "                                'totale_casi', 'nuovi_positivi', 'deceduti'], dtype={'codice_regione': str,\\\n",
    "                                'denominazione_regione': str, 'totale_casi': int, 'nuovi_positivi': int, 'deceduti': int},\\\n",
    "                                parse_dates=['data'])\n",
    "    result = result.rename(columns={'codice_regione': 'local_id', 'totale_casi': 'total_cases',\\\n",
    "                              'nuovi_positivi': 'daily_cases', 'deceduti': 'total_deaths'})\n",
    "    result['country_id'] = 'IT'\n",
    "    result['date'] = result['data'].dt.normalize()\n",
    "    result = result.drop(['data'], axis=1)\n",
    "    result = result.replace({'local_id': {'21': '04', '22': '04'}})\n",
    "    result = result.groupby(['local_id', 'date']).sum().reset_index()\n",
    "    result['daily_deaths'] = result.apply(lambda x: x['total_deaths'] - result['total_deaths'].loc[(result['date'] == x['date'] + pd.DateOffset(-1)) &\\\n",
    "                                                                            (result['local_id'] == x['local_id'])].max(), axis=1)\n",
    "    \n",
    "    italy_pop_file_path = os.path.join(curr_dir, 'raw_covid_data/italy/italy_population.csv')\n",
    "    pop_raw = pd.read_csv(italy_pop_file_path, index_col=None, usecols=['local_id', 'total'], dtype={'local_id': str, 'total': int})\n",
    "    pop_raw = pop_raw.rename(columns={'total': 'population'})\n",
    "    \n",
    "    result = result.merge(pop_raw, how='left', left_on='local_id', right_on='local_id')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#US total cases reach 100 on 2020-03-04\n",
    "\n",
    "def us_covid():\n",
    "    #Cases unused cols...\n",
    "    #UID,iso2,iso3,code3,FIPS,Admin2,Country_Region,Lat,Long_,Combined_Key\n",
    "    us_cases_file_path = os.path.join(curr_dir, 'raw_covid_data/us/us_covid_raw_cases.csv')\n",
    "    us_cases_raw = pd.read_csv(us_cases_file_path, index_col=None)\n",
    "    us_cases_raw_filtered = us_cases_raw.drop(['UID','iso2','iso3','code3','FIPS','Admin2',\\\n",
    "                                               'Country_Region','Lat','Long_','Combined_Key'], axis=1)\n",
    "    us_cases_raw_grouped = us_cases_raw_filtered.groupby(['Province_State']).sum().reset_index()\n",
    "    us_cases_df = pd.DataFrame(columns=['region_name', 'date', 'total_cases'])\n",
    "    for state in id_mappings.us:\n",
    "        state_cases = us_cases_raw_grouped.loc[us_cases_raw_grouped['Province_State'] == state]\n",
    "        state_transposed = state_cases.drop(['Province_State'], axis=1).T.reset_index()\n",
    "        state_renamed = state_transposed.rename(columns={state_transposed.columns[0]: 'date_raw', state_transposed.columns[1]: 'total_cases'})\n",
    "        state_renamed['region_name'] = state\n",
    "        state_renamed['date'] = pd.to_datetime(state_renamed['date_raw'], format='%m/%d/%y')\n",
    "        state_final = state_renamed.drop(['date_raw'], axis=1)\n",
    "        us_cases_df = us_cases_df.append(state_final)\n",
    "    us_cases_df = us_cases_df.set_index(['date', 'region_name'])\n",
    "\n",
    "\n",
    "    #Deaths unused cols...\n",
    "    #UID,iso2,iso3,code3,FIPS,Admin2,Country_Region,Lat,Long_,Combined_Key,Population\n",
    "    us_deaths_file_path = os.path.join(curr_dir, 'raw_covid_data/us/us_covid_raw_deaths.csv')\n",
    "    us_deaths_raw = pd.read_csv(us_deaths_file_path, index_col=None)\n",
    "    us_deaths_raw_filtered = us_deaths_raw.drop(['UID','iso2','iso3','code3','FIPS','Admin2','Country_Region',\\\n",
    "                                                 'Lat','Long_','Combined_Key','Population'], axis=1)\n",
    "    us_deaths_raw_grouped = us_deaths_raw_filtered.groupby(['Province_State']).sum().reset_index()\n",
    "    us_deaths_df = pd.DataFrame(columns=['region_name', 'date', 'total_deaths'])\n",
    "    for state in id_mappings.us:\n",
    "        state_deaths = us_deaths_raw_grouped.loc[us_deaths_raw_grouped['Province_State'] == state]\n",
    "        state_transposed = state_deaths.drop(['Province_State'], axis=1).T.reset_index()\n",
    "        state_renamed = state_transposed.rename(columns={state_transposed.columns[0]: 'date_raw', state_transposed.columns[1]: 'total_deaths'})\n",
    "        state_renamed['region_name'] = state\n",
    "        state_renamed['date'] = pd.to_datetime(state_renamed['date_raw'], format='%m/%d/%y')\n",
    "        state_final = state_renamed.drop(['date_raw'], axis=1)\n",
    "        us_deaths_df = us_deaths_df.append(state_final)\n",
    "    us_deaths_df = us_deaths_df.set_index(['date', 'region_name'])\n",
    "\n",
    "    result = us_cases_df.merge(us_deaths_df, how='left', left_index=True, right_index=True).reset_index()\n",
    "    \n",
    "    result['daily_cases'] = result.apply(lambda x: x['total_cases'] - result['total_cases'].loc[(result['date'] == x['date'] + pd.DateOffset(-1)) &\\\n",
    "                                                                        (result['region_name'] == x['region_name'])].max(), axis=1)\n",
    "    \n",
    "    result['daily_deaths'] = result.apply(lambda x: x['total_deaths'] - result['total_deaths'].loc[(result['date'] == x['date'] + pd.DateOffset(-1)) &\\\n",
    "                                                                        (result['region_name'] == x['region_name'])].max(), axis=1)\n",
    "    \n",
    "    #Add population data\n",
    "    us_population_data = us_deaths_raw[['Province_State', 'Population']].groupby(['Province_State']).sum().reset_index()\n",
    "    us_population_data = us_population_data.rename(columns={'Province_State': 'region_name', 'Population': 'population'})\n",
    "    result = result.merge(us_population_data, how='left', left_on='region_name', right_on='region_name')\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spanish data has iso-8859-1 encoding\n",
    "#Date is DD/MM for using dayfirst=True when reading csv\n",
    "#Spainsh raw data file contains a series of notes at the end within the csv\n",
    "#Data provided contains total running deaths and cases\n",
    "\n",
    "def spain_covid():\n",
    "    spain_file_path = os.path.join(curr_dir, 'raw_covid_data/spain/spain_covid_raw.csv')\n",
    "    result = pd.read_csv(spain_file_path, encoding='iso-8859-1', index_col=None, usecols=['CCAA','FECHA','PCR+','Fallecidos'],\\\n",
    "                         dtype={'CCAA': str, 'PCR+': float, 'Fallecidos': float}, parse_dates=['FECHA'], dayfirst=True)\n",
    "    result = result.rename(columns={'CCAA': 'local_id', 'FECHA': 'date', 'PCR+': 'total_cases', 'Fallecidos': 'total_deaths'})\n",
    "    #Remove comments from csv\n",
    "    result = result.loc[result['date'].notna() == True]\n",
    "    result['daily_cases'] = result.apply(lambda x: x['total_cases'] - result['total_cases'].loc[(result['date'] == x['date'] + pd.DateOffset(-1)) &\\\n",
    "                                                                        (result['local_id'] == x['local_id'])].max(), axis=1)\n",
    "    \n",
    "    result['daily_deaths'] = result.apply(lambda x: x['total_deaths'] - result['total_deaths'].loc[(result['date'] == x['date'] + pd.DateOffset(-1)) &\\\n",
    "                                                                        (result['local_id'] == x['local_id'])].max(), axis=1)\n",
    "    \n",
    "    #Add population data\n",
    "    spain_pop_file_path = os.path.join(curr_dir, 'raw_covid_data/spain/spain_population.csv')\n",
    "    pop_raw = pd.read_csv(spain_pop_file_path, index_col=None, usecols=['local_id', 'population'], dtype={'local_id': str, 'population': int})\n",
    "    result = result.merge(pop_raw, how='left', left_on='local_id', right_on='local_id')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google trends is using the pre-2016 French regional system\n",
    "#...COVID sata rebuilt to pre-2016 regions by departement\n",
    "#Unable to find French daily mortility statistics - only z-scores for execss death by post-2016 region available\n",
    "\n",
    "def france_covid():\n",
    "    france_file_path = os.path.join(curr_dir, 'raw_covid_data/france/france_covid_raw.csv')\n",
    "    result = pd.read_csv(france_file_path, index_col=None, usecols=['dep', 'date_de_passage', 'sursaud_cl_age_corona', 'nbre_pass_corona'], dtype={'dep': str,\\\n",
    "                        'sursaud_cl_age_corona': str, 'nbre_pass_corona': float},\\\n",
    "                        parse_dates=['date_de_passage'])\n",
    "    sub_regions = list(id_mappings.france_sub_regions.keys())\n",
    "    result = result.loc[result['dep'].isin(sub_regions)]\n",
    "    result = result.rename(columns={'date_de_passage': 'date', 'nbre_pass_corona': 'daily_cases'})\n",
    "    result['iso_id'] = result.apply(lambda x: id_mappings.france_sub_regions[x['dep']], axis=1)\n",
    "    result = result.loc[result['sursaud_cl_age_corona'] =='0']\n",
    "    result = result[['iso_id', 'date', 'daily_cases']].groupby(['iso_id', 'date']).sum().reset_index()\n",
    "    result['total_cases'] = result.apply(lambda x: result['daily_cases'].loc[(result['date'] <= x['date']) &\\\n",
    "                                                            (result['iso_id'] == x['iso_id'])].sum(), axis=1)\n",
    "    result['daily_deaths'] = np.nan\n",
    "    result['total_deaths'] = np.nan\n",
    "    \n",
    "    france_pop_file_path = os.path.join(curr_dir, 'raw_covid_data/france/france_population.csv')\n",
    "    pop_raw = pd.read_csv(france_pop_file_path, index_col=None, usecols=['departement', 'total'], dtype={'departement': str, 'total': int})\n",
    "    pop_raw = pop_raw.loc[pop_raw['departement'].isin(sub_regions)]\n",
    "    pop_raw['iso_id'] = pop_raw.apply(lambda x:id_mappings.france_sub_regions[x['departement']], axis=1)\n",
    "    pop_grouped = pop_raw[['iso_id', 'total']].groupby(['iso_id']).sum().reset_index()\n",
    "    pop_grouped = pop_grouped.rename(columns={'total': 'population'})\n",
    "    \n",
    "    result = result.merge(pop_grouped, left_on='iso_id', right_on='iso_id')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data as large excel file with national, state and municipal data\n",
    "#National data has state id and municipal id fields missing\n",
    "#State level has municipal id field missing\n",
    "#Population data from 2019 census is contained within the COVID-19 excel\n",
    "\n",
    "def brazil_covid():\n",
    "    brazil_file_path = os.path.join(curr_dir, 'raw_covid_data/brazil/brazil_covid_raw.xlsx')\n",
    "    raw_df = pd.read_excel(brazil_file_path, index_col=None, header=0, usecols=['estado', 'codmun', 'data',\\\n",
    "                            'populacaoTCU2019', 'casosAcumulado', 'obitosAcumulado'], dtype={'estado': str,\\\n",
    "                            'codmun': str, 'populacaoTCU2019': float, 'casosAcumulado': int, 'obitosAcumulado': int}, parse_dates=['data'])\n",
    "    raw_df = raw_df.loc[(raw_df['estado'].isna() == False) & (raw_df['codmun'].isna() == True)]\n",
    "    raw_df = raw_df.drop(['codmun'], axis=1)\n",
    "    raw_df = raw_df.rename(columns={'estado': 'local_id', 'data': 'date', 'populacaoTCU2019': 'population',\\\n",
    "                                    'casosAcumulado': 'total_cases', 'obitosAcumulado': 'total_deaths'})\n",
    "    \n",
    "    result = pd.DataFrame(columns=['local_id', 'date', 'population', 'total_cases', 'total_deaths'])\n",
    "    set_dates = pd.DataFrame(pd.date_range(start='2020-02-26', end='2020-05-17', name='date'), columns=['date'])\n",
    "    set_dates = set_dates.reset_index(drop=True)\n",
    "    regions = raw_df['local_id'].unique()\n",
    "    for r in regions:\n",
    "        region_data = raw_df.loc[raw_df['local_id'] == r]\n",
    "        fill_dates = set_dates.merge(region_data, how='left', left_on='date', right_on='date')\n",
    "        fill_dates['local_id'] = r\n",
    "        fill_dates['population'] = region_data['population'].max()\n",
    "        fill_dates = fill_dates.fillna(0)\n",
    "        result = result.append(fill_dates)\n",
    "        \n",
    "    result['daily_cases'] = result.apply(lambda x: x['total_cases'] - result['total_cases'].loc[(result['date'] == x['date'] + pd.DateOffset(-1)) &\\\n",
    "                                                                        (result['local_id'] == x['local_id'])].max(), axis=1)\n",
    "    \n",
    "    result['daily_deaths'] = result.apply(lambda x: x['total_deaths'] - result['total_deaths'].loc[(result['date'] == x['date'] + pd.DateOffset(-1)) &\\\n",
    "                                                                        (result['local_id'] == x['local_id'])].max(), axis=1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format of start/end date 'yyyy-mm-dd'\n",
    "def get_search_data(phrase_list, regions, country_code, start_day, end_day):\n",
    "    date_range = start_day + ' ' + end_day\n",
    "    counter = 0\n",
    "    results = pd.DataFrame()\n",
    "    set_dates = pd.DataFrame(pd.date_range(start=start_day, end=end_day, name='date').date, columns=['date'])\n",
    "    for r in regions:\n",
    "        df_builder = set_dates\n",
    "        df_builder['country'] = country_code\n",
    "        df_builder['iso_id'] = regions[r]['iso_id']\n",
    "        df_builder['region_name'] = r\n",
    "        try:\n",
    "            df_builder['local_id'] = regions[r]['local_id']\n",
    "        except:\n",
    "            df_builder['local_id'] = np.nan \n",
    "        df_builder = df_builder.set_index(['date', 'iso_id'])\n",
    "        results = results.append(df_builder)\n",
    "    \n",
    "    for p in phrase_list:\n",
    "        search_term = phrase_list[p]\n",
    "        phrase_results = pd.DataFrame()\n",
    "        weightings = pd.DataFrame()\n",
    "        for r in regions:\n",
    "            geo_id = regions[r]['iso_id']\n",
    "            pytrend.build_payload([search_term], timeframe=date_range, geo=geo_id)\n",
    "            trends_result = pytrend.interest_over_time()\n",
    "            trends_result = trends_result.rename(columns={search_term: p})\n",
    "            try:\n",
    "                trends_result = trends_result.drop(['isPartial'], axis=1)\n",
    "            except:\n",
    "                pass\n",
    "            trends_result['iso_id'] = geo_id\n",
    "            trends_result = trends_result.set_index([trends_result.index, 'iso_id'])\n",
    "            phrase_results = phrase_results.append(trends_result)\n",
    "            time.sleep(4)\n",
    "            counter += 1\n",
    "            print(counter, ' / ', (len(phrase_list) * (len(regions) + set_dates['date'].count())))\n",
    "        for day in set_dates['date']:\n",
    "            day_end = (day + pd.DateOffset(1))\n",
    "            start_string = day.strftime('%Y-%m-%d')\n",
    "            end_string = day_end.strftime('%Y-%m-%d')\n",
    "            day_range = start_string + ' ' + end_string\n",
    "            pytrend.build_payload([search_term], timeframe=day_range, geo=country_code)\n",
    "            daily_weighting = pytrend.interest_by_region(resolution='REGION', inc_low_vol=True, inc_geo_code=True)\n",
    "            daily_weighting['date'] = day\n",
    "            phrase_col_name = p + '_daily_weighting'\n",
    "            daily_weighting = daily_weighting.rename(columns={search_term: phrase_col_name})\n",
    "            daily_weighting = daily_weighting.rename(columns={'geoCode': 'iso_id'})\n",
    "            daily_weighting = daily_weighting.set_index(['iso_id', 'date'])\n",
    "            weightings = weightings.append(daily_weighting)\n",
    "            time.sleep(2)\n",
    "            counter += 1\n",
    "            print(counter, ' / ', (len(phrase_list) * (len(regions) + set_dates['date'].count())))\n",
    "        phrase_results = phrase_results.merge(weightings, how='left', left_index=True, right_index=True)\n",
    "        results = results.merge(phrase_results, how='left', left_index=True, right_index=True)\n",
    "    return results.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Italy\n",
    "def italy_raw():\n",
    "    italy_covid_raw = italy_covid()\n",
    "    italy_search_interest = get_search_data(translations.italy, id_mappings.italy, 'IT', '2020-02-24', '2020-05-17')\n",
    "    result = italy_search_interest.merge(italy_covid_raw, how='left', left_on=['local_id', 'date'], right_on=['local_id', 'date'])\n",
    "    result.to_csv(os.path.join(os.path.dirname(curr_dir), 'analysis/italy_full.csv'), index=False)\n",
    "    return result\n",
    "\n",
    "#US\n",
    "def us_raw():\n",
    "    us_covid_raw = us_covid()\n",
    "    us_search_interest = get_search_data(translations.us, id_mappings.us, 'US', '2020-02-21', '2020-05-17')\n",
    "    result = us_search_interest.merge(us_covid_raw, how='left', left_on=['date', 'region_name'], right_on=['date', 'region_name'])\n",
    "    result.to_csv(os.path.join(os.path.dirname(curr_dir), 'analysis/us_full.csv'), index=False)\n",
    "    return result\n",
    "\n",
    "#Spain\n",
    "def spain_raw():\n",
    "    spain_covid_raw = spain_covid()\n",
    "    spain_search_interest = get_search_data(translations.spain, id_mappings.spain, 'ES', '2020-02-20', '2020-05-17')\n",
    "    result = spain_search_interest.merge(spain_covid_raw, how='left', left_on=['date', 'local_id'], right_on=['date', 'local_id'])\n",
    "    result.to_csv(os.path.join(os.path.dirname(curr_dir), 'analysis/spain_full.csv'), index=False)\n",
    "    return result\n",
    "\n",
    "#France\n",
    "def france_raw():\n",
    "    france_covid_raw = france_covid()\n",
    "    france_search_interest = get_search_data(translations.france, id_mappings.france, 'FR', '2020-02-24', '2020-05-17')\n",
    "    result = france_search_interest.merge(france_covid_raw, how='left', left_on=['date', 'iso_id'], right_on=['date', 'iso_id'])\n",
    "    result.to_csv(os.path.join(os.path.dirname(curr_dir), 'analysis/france_full.csv'), index=False)\n",
    "    return result\n",
    "\n",
    "#Brazil\n",
    "def brazil_raw():\n",
    "    brazil_covid_raw = brazil_covid()\n",
    "    brazil_search_interest = get_search_data(translations.brazil, id_mappings.brazil, 'BR', '2020-02-26', '2020-05-17')\n",
    "    result = brazil_search_interest.merge(brazil_covid_raw, how='left', left_on=['date', 'local_id'], right_on=['date', 'local_id'])\n",
    "    result.to_csv(os.path.join(os.path.dirname(curr_dir), 'analysis/brazil_full.csv'), index=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum number of API requests in 24hrs for Google Trends is 1600\n",
    "#Check request numbers before attempting to use all raw data functions on the same day\n",
    "\n",
    "#Uncomment a line below to fetch the raw data and save to the 'analysis' folder, for the respective country\n",
    "#italy_raw()\n",
    "#us_raw()\n",
    "#spain_raw()\n",
    "#france_raw()\n",
    "#brazil_raw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
