{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Setup\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure raw data files have been generated using \"process_raw_data\" notebook in \"anlysis\" folder\n",
    "\n",
    "italy = pd.read_csv('italy_full.csv', parse_dates=['date'])\n",
    "us = pd.read_csv('us_full.csv', parse_dates=['date'])\n",
    "spain = pd.read_csv('spain_full.csv', parse_dates=['date'])\n",
    "france = pd.read_csv('france_full.csv', parse_dates=['date'])\n",
    "brazil = pd.read_csv('brazil_full.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_covid_per_million(df):\n",
    "    df['daily_cases_per_million'] = (1000000 / df['population']) * df['daily_cases']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all coutries before making the scaled interest colunms\n",
    "\n",
    "def make_scaled_search_interest(df):\n",
    "    df['weighted_loss_smell'] = df['loss_smell'] * (df['loss_smell_daily_weighting'] / 100)\n",
    "    df['weighted_sense_smell'] = df['sense_smell'] * (df['sense_smell_daily_weighting'] / 100)\n",
    "    df['weighted_loss_taste'] = df['loss_taste'] * (df['loss_taste_daily_weighting'] / 100)\n",
    "    df['weighted_sense_taste'] = df['sense_taste'] * (df['sense_taste_daily_weighting'] / 100)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving mean to be lead 7 day for both search interest and cases for consistency\n",
    "#Centered mean wouldadd data from the future into the daily data so cannot be used\n",
    "\n",
    "def make_rolling_mean(df):\n",
    "    col_list = ['loss_smell',\\\n",
    "        'sense_smell', \\\n",
    "        'loss_taste',\\\n",
    "        'sense_taste',\\\n",
    "        'weighted_loss_smell',\\\n",
    "        'weighted_sense_smell', \\\n",
    "        'weighted_loss_taste',\\\n",
    "        'weighted_sense_taste',\\\n",
    "        'daily_cases',\\\n",
    "        'daily_cases_per_million']\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    region_list = df['iso_id'].unique()\n",
    "    prefix = '7d_MM_'\n",
    "    for r in region_list:\n",
    "        df_region = df.loc[df['iso_id'] == r].copy()\n",
    "        df_region = df_region.sort_values(by='date', ascending=True)\n",
    "        df_region = df_region.set_index('date')\n",
    "        for col in col_list:\n",
    "            new_col_name = prefix + col\n",
    "            df_region[new_col_name] = df_region[col].rolling(window=7).mean()\n",
    "        result = result.append(df_region.reset_index())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculated_variables(df):\n",
    "    result = make_covid_per_million(df)\n",
    "    result = make_scaled_search_interest(result)\n",
    "    result = make_rolling_mean(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "italy = calculated_variables(italy)\n",
    "us = calculated_variables(us)\n",
    "spain = calculated_variables(spain)\n",
    "france = calculated_variables(france)\n",
    "brazil = calculated_variables(brazil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_all():\n",
    "    fig, ax = plt.subplots(4, 5, figsize=(30,20))\n",
    "    \n",
    "    countries_list = [italy, us, spain, france, brazil]\n",
    "    \n",
    "    for i, c in enumerate(countries_list):\n",
    "        df = c\n",
    "        ax[0,i].scatter(df['7d_MM_daily_cases_per_million'], df['7d_MM_weighted_loss_smell'], marker='x', color='#000000')\n",
    "        ax[0,i].set_title('Loss of sense of smell', size=15)\n",
    "        ax[0,i].set_ylim(0,80)\n",
    "        ax[0,i].set_xlim(0,500)\n",
    "        ax[0,i].tick_params(axis='y', labelsize=15)\n",
    "        ax[0,i].yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "        ax[0,i].set_ylabel('Search interest (7 day moving mean)', size=15)\n",
    "        ax[0,i].tick_params(axis='x', labelsize=15)\n",
    "        ax[0,i].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "        ax[0,i].set_xlabel('New daily cases per million (7 day moving mean)', size=15)\n",
    "\n",
    "        ax[1,i].scatter(df['7d_MM_daily_cases_per_million'], df['7d_MM_weighted_loss_taste'], marker='x', color='#000000')\n",
    "        ax[1,i].set_title('Loss of sense of taste', size=25)\n",
    "        ax[1,i].set_ylim(0,80)\n",
    "        ax[1,i].set_xlim(0,500)\n",
    "        ax[1,i].tick_params(axis='y', labelsize=15)\n",
    "        ax[1,i].yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "        ax[1,i].set_ylabel('Search interest (7 day moving mean)', size=15)\n",
    "        ax[1,i].tick_params(axis='x', labelsize=15)\n",
    "        ax[1,i].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "        ax[1,i].set_xlabel('New daily cases per million (7 day moving mean)', size=15)\n",
    "\n",
    "        ax[2,i].scatter(df['7d_MM_daily_cases_per_million'], df['7d_MM_weighted_sense_smell'], marker='x', color='#000000')\n",
    "        ax[2,i].set_title('Sense of smell', size=25)\n",
    "        ax[2,i].set_ylim(0,80)\n",
    "        ax[2,i].set_xlim(0,500)\n",
    "        ax[2,i].tick_params(axis='y', labelsize=15)\n",
    "        ax[2,i].yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "        ax[2,i].set_ylabel('Search interest (7 day moving mean)', size=15)\n",
    "        ax[2,i].tick_params(axis='x', labelsize=15)\n",
    "        ax[2,i].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "        ax[2,i].set_xlabel('New daily cases per million (7 day moving mean)', size=15)\n",
    "\n",
    "        ax[3,i].scatter(df['7d_MM_daily_cases_per_million'], df['7d_MM_weighted_sense_taste'], marker='x', color='#000000')\n",
    "        ax[3,i].set_title('Sense of taste', size=25)\n",
    "        ax[3,i].set_ylim(0,80)\n",
    "        ax[3,i].set_xlim(0,500)\n",
    "        ax[3,i].tick_params(axis='y', labelsize=15)\n",
    "        ax[3,i].yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "        ax[3,i].set_ylabel('Search interest (7 day moving mean)', size=15)\n",
    "        ax[3,i].tick_params(axis='x', labelsize=15)\n",
    "        ax[3,i].xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "        ax[3,i].set_xlabel('New daily cases per million (7 day moving mean)', size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to view scatter plot to visualise trends\n",
    "#plot_scatter_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kolmogorov-Smirnov testing for all regions in a country per search concept\n",
    "#example usage: ks(italy, 'loss_smell')\n",
    "\n",
    "def ks(df, concept):\n",
    "    \n",
    "    interest_types = {'loss_smell': '7d_MM_loss_smell',\n",
    "                     'sense_smell': '7d_MM_sense_smell',\n",
    "                     'loss_taste': '7d_MM_loss_taste',\n",
    "                     'sense_taste': '7d_MM_sense_taste'}\n",
    "    \n",
    "    interest_type = interest_types[concept]\n",
    "    \n",
    "    result = pd.DataFrame(columns=['iso_id', 'D', 'p_value'])\n",
    "    regions_sorted = df.sort_values(by='daily_cases', ascending=False)\n",
    "    regions_sort_list = regions_sorted['iso_id'].unique()\n",
    "    for r in regions_sort_list:\n",
    "        try:\n",
    "            res = stats.kstest(df[interest_type].loc[df['iso_id'] == r].dropna(), 'norm')\n",
    "            result = result.append({'iso_id': r, 'D': res.statistic, 'p_value': res.pvalue}, ignore_index=True)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform Spearman rank on all regions in a country for a single search term\n",
    "#Set prettify to True to truncate Rs and p_value at 3dp\n",
    "def spearmanr_cases(df, concept, prettify):\n",
    "    \n",
    "    interest_types = {'loss_smell': '7d_MM_loss_smell',\n",
    "                     'sense_smell': '7d_MM_sense_smell',\n",
    "                     'loss_taste': '7d_MM_loss_taste',\n",
    "                     'sense_taste': '7d_MM_sense_taste'}\n",
    "    \n",
    "    interest_type = interest_types[concept]\n",
    "    \n",
    "    result = pd.DataFrame(columns=['iso_id', 'region', 'search_term', 'r_s', 'p_value'])\n",
    "    regions_sorted = df.sort_values(by='daily_cases', ascending=False)\n",
    "    regions_sort_list = regions_sorted['iso_id'].unique()\n",
    "    for r in regions_sort_list:\n",
    "        try:\n",
    "            res = stats.spearmanr(df['7d_MM_daily_cases_per_million'].loc[df['iso_id'] == r], df[interest_type].loc[df['iso_id'] == r], nan_policy='omit')\n",
    "            result = result.append({'iso_id': r, 'region': df['region_name'].loc[df['iso_id'] == r][0], 'r_s': res.correlation, 'p_value': res.pvalue}, ignore_index=True)\n",
    "        except:\n",
    "            result = result.append({'iso_id': r, 'region': df['region_name'].loc[df['iso_id'] == r][0], 'r_s': np.NaN, 'p_value': np.NaN}, ignore_index=True)\n",
    "    \n",
    "    result['search_term'] = concept\n",
    "    result['country'] = df['country'].iloc[0]\n",
    "    \n",
    "    if prettify == True:\n",
    "        result['r_s'] = result['r_s'].apply(lambda x: x if(x == np.NaN) else np.round(x, 3))\n",
    "        result['p_value'] = result['p_value'].apply(lambda x: x if(x == np.NaN) else ('<0.001' if(x < 0.001) else np.round(x, 3)))\n",
    "    \n",
    "    return result\n",
    "\n",
    "#Aggregation function to loop through and combine all regions in all countries for all search terms\n",
    "def agg_spearmanr():\n",
    "    countries_list = [italy, us, spain, france, brazil]\n",
    "    search_terms = ['loss_smell', 'loss_taste', 'sense_smell', 'sense_taste']\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    for c in countries_list:\n",
    "        for t in search_terms:\n",
    "            summary = spearmanr_cases(c, t, False)\n",
    "            results = results.append(summary)\n",
    "    \n",
    "    results = results.reset_index(drop=True)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to count number of regions in each result category for Spearman rank (per country and search term)\n",
    "#Result categories are:\n",
    "#...Strongly positive Rs (>0.65) and significant (<0.05)\n",
    "#...Medium positive Rs (>0.3) and significant (<0.05)\n",
    "#...Weakly positive Rs (>0) and significant (<0.05)\n",
    "#...Weakly negative Rs (<0) and significant (<0.05)\n",
    "#...Medium negative Rs (<-0.3) and significant (<0.05)\n",
    "#...Strongly negative Rs (<-0.65) and significant (<0.05)\n",
    "#...Postive Rs (>0) and not significant (>=0.05)\n",
    "#...Negative Rs (<0) and not significant (>=0.05)\n",
    "#...NaN: Insufficient data for test\n",
    "\n",
    "def spearmanr_summary_helper(df, concept):\n",
    "    \n",
    "    interest_types = {'loss_smell': '7d_MM_loss_smell',\n",
    "                     'sense_smell': '7d_MM_sense_smell',\n",
    "                     'loss_taste': '7d_MM_loss_taste',\n",
    "                     'sense_taste': '7d_MM_sense_taste'}\n",
    "    \n",
    "    interest_type = interest_types[concept]\n",
    "    \n",
    "    result = pd.DataFrame(columns=['iso_id', 'region', 'r_s', 'p_value'])\n",
    "    regions_sorted = df.sort_values(by='daily_cases', ascending=False)\n",
    "    regions_sort_list = regions_sorted['iso_id'].unique()\n",
    "    for r in regions_sort_list:\n",
    "        try:\n",
    "            res = stats.spearmanr(df['7d_MM_daily_cases_per_million'].loc[df['iso_id'] == r], df[interest_type].loc[df['iso_id'] == r], nan_policy='omit')\n",
    "            result = result.append({'iso_id': r, 'region': df['region_name'].loc[df['iso_id'] == r][0], 'r_s': res.correlation, 'p_value': res.pvalue}, ignore_index=True)\n",
    "        except:\n",
    "            result = result.append({'iso_id': r, 'region': df['region_name'].loc[df['iso_id'] == r][0], 'r_s': np.NaN, 'p_value': np.NaN}, ignore_index=True)\n",
    "    \n",
    "    data = {'country':[df['country'].iloc[0]],\\\n",
    "            'search_term':[concept],\\\n",
    "            'strong_pos_sig':[result['region'].loc[(result['r_s'] >= 0.65) & (result['p_value'] < 0.05)].count()],\\\n",
    "            'mod_pos_sig':[result['region'].loc[(result['r_s'] >= 0.3) & (result['r_s'] < 0.65) & (result['p_value'] < 0.05)].count()],\\\n",
    "            'weak_pos_sig':[result['region'].loc[(result['r_s'] >= 0) & (result['r_s'] < 0.3) & (result['p_value'] < 0.05)].count()],\\\n",
    "            'weak_neg_sig':[result['region'].loc[(result['r_s'] >= -0.3) & (result['r_s'] < 0) & (result['p_value'] < 0.05)].count()],\\\n",
    "            'mod_neg_sig':[result['region'].loc[(result['r_s'] >= -0.65) & (result['r_s'] < -0.3) & (result['p_value'] < 0.05)].count()],\\\n",
    "            'strong_neg_sig':[result['region'].loc[(result['r_s'] < -0.65) & (result['p_value'] < 0.05)].count()],\\\n",
    "            'pos_non_sig':[result['region'].loc[(result['r_s'] >= 0) & (result['p_value'] >= 0.05)].count()],\\\n",
    "            'neg_non_sig':[result['region'].loc[(result['r_s'] < 0) & (result['p_value'] >= 0.05)].count()],\\\n",
    "            'nan':[result['region'].loc[result['r_s'].isna()].count()]}\n",
    "    \n",
    "    result_summary = pd.DataFrame(data)\n",
    "    \n",
    "    return result_summary\n",
    "\n",
    "#Helper function to build a summary table of Spearman rank categorised results\n",
    "def make_summary_table():\n",
    "    countries_list = [italy, us, spain, france, brazil]\n",
    "    search_terms = ['loss_smell', 'loss_taste', 'sense_smell', 'sense_taste']\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    for c in countries_list:\n",
    "        for t in search_terms:\n",
    "            summary = spearmanr_summary_helper(c, t)\n",
    "            results = results.append(summary)\n",
    "    \n",
    "    results = results.reset_index(drop=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pools all regional data from a single country for a search term, and performs Spearman rank...\n",
    "#... to look at relative search interest between regions\n",
    "def national_grouping():\n",
    "    countries_list = [italy, us, spain, france, brazil]\n",
    "    search_terms = {'loss_smell': '7d_MM_loss_smell',\n",
    "                 'sense_smell': '7d_MM_sense_smell',\n",
    "                 'loss_taste': '7d_MM_loss_taste',\n",
    "                 'sense_taste': '7d_MM_sense_taste'}\n",
    "    \n",
    "    results = pd.DataFrame(columns=['country', 'search_term', 'r_s', 'p_val'])\n",
    "    \n",
    "    for c in countries_list:\n",
    "        for t in search_terms:\n",
    "            res = stats.spearmanr(c['7d_MM_daily_cases_per_million'], c[search_terms[t]], nan_policy='omit')\n",
    "            r_s = res.correlation\n",
    "            p_val = res.pvalue\n",
    "            row = {'country': c['country'].iloc[0], 'search_term': t, 'r_s': r_s, 'p_val': p_val}\n",
    "            results = results.append(row, ignore_index=True)\n",
    "    \n",
    "    results = results.reset_index(drop=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for normal distribution\n",
    "#ks(_country_, 'loss_smell')\n",
    "#ks(_country_, 'loss_taste')\n",
    "#ks(_country_, 'sense_smell')\n",
    "#ks(_country_, 'sense_taste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final result set\n",
    "within_region_results = agg_spearmanr()\n",
    "within_region_summary = make_summary_table()\n",
    "pooled_relative_results = national_grouping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment lines below to write results\n",
    "#within_region_results.to_csv('within_region_full_results.csv', index=False)\n",
    "#within_region_summary.to_csv('within_region_summary.csv', index=False)\n",
    "#pooled_relative_results.to_csv('pooled_relative_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
